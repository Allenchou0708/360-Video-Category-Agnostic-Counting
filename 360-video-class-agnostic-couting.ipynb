{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"collapsed_sections":["OBbic5mzefcj","34hSELGqeoxz","wDgKha5fe5bZ","jmdkNYFBfEc6","uwh-wy08fTgn","w9ja742P_Z-H"],"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13276876,"sourceType":"datasetVersion","datasetId":8413926},{"sourceId":610468,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":458468,"modelId":474384},{"sourceId":610470,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":458469,"modelId":474385},{"sourceId":611125,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":459024,"modelId":474908}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n    Generate cat2imgkey from /kaggle/input/[selected_dataset]\n    cat2imgkey : {\"category\":[imgkey1, imgkey2 ...]}\n\"\"\"\n\nimport os\n\nbase_dir = \"/kaggle/input/pandora-clean-straight-dataset-correct/360 CAC/query_images\"\ncats = os.listdir(base_dir)  \ncat2imgkey = {}\n\nfor cat in cats:\n    cat_path = os.path.join(base_dir, cat)\n    if os.path.isdir(cat_path):\n        imgkeys = [d for d in os.listdir(cat_path) if os.path.isdir(os.path.join(cat_path, d))]\n        cat2imgkey[cat] = imgkeys\n\n\ntrain_cat = ['computer', 'car', 'chair', 'window', 'book', 'cabinet']\nval_cat = ['door', 'penguin', 'bottle']\ntest_cat = ['dog', 'hyaenidae', 'boat', 'person']\n\n\n\ntrain_data_num = 0\nval_data_num = 0\ntest_data_num = 0\n\n\nfor cat in train_cat:\n  train_data_num += len(cat2imgkey[cat])\n\nfor cat in val_cat:\n  val_data_num += len(cat2imgkey[cat])\n\nfor cat in test_cat:\n  test_data_num += len(cat2imgkey[cat])\n\nprint(f\"train data num : {train_data_num}\")\nprint(f\"val data num : {val_data_num}\")\nprint(f\"test data num : {test_data_num}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n    Download CFOCNet background repository for the environment\n\"\"\"\n\n!git clone https://github.com/Allenchou0708/My-Class-agnostic-Few-shot-Object-Counting.git\n!mv My-Class-agnostic-Few-shot-Object-Counting/* .","metadata":{"id":"CDTgFolqbM0E","outputId":"394dac50-fe6e-4afd-f523-897a3fb64b30","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n    Download SSIM package\n\"\"\"\n\n!pip install pytorch_msssim","metadata":{"id":"ywU-nn9NLlw_","outputId":"cf086fd3-ce20-415a-968c-df2f4f9efa6c","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile ./model/resblocks.py\n\n\"\"\"\n    Revise the Resnet in CFOCNet for processing deform feature\n\"\"\"\n\n\nimport torch\nfrom torchvision.models.resnet import resnet50\nimport torch.nn as nn\n\ndef make_resblocks(deform_feature, data_pipe = \"reference\"):\n    \n\n    net = resnet50(pretrained=False)\n\n\n    \"\"\"\n        If it is the query data pipe, \n        we replace the first conv with the conv accepted 4 channel\n    \"\"\"\n    if data_pipe == \"query\" and deform_feature : \n\n        new_conv = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)  \n        net.conv1 = new_conv  # Replace in ResNet\n    \n    \n    layer0_name = ['conv1','bn1','relu','maxpool']\n    layer1_name = ['layer1']\n    layer2_name = ['layer2']\n    layer3_name = ['layer3']\n\n    layer0 = nn.Sequential()\n    layer1 = nn.Sequential()\n    layer2 = nn.Sequential()\n    layer3 = nn.Sequential()\n\n    for n,c in net.named_children():\n        if n in layer0_name:\n            layer0.add_module(n,c)\n        elif n in layer1_name:\n            layer1.add_module(n,c)\n        elif n in layer2_name:\n            layer2.add_module(n,c)\n        elif n in layer3_name:\n            layer3 = c\n        else:\n            break\n\n    return layer0, layer1, layer2, layer3\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile ./model/CFOCNet.py\n\n\"\"\"\n    Replace some module in the original CFOCNet\n\"\"\"\n\n\nimport torch\nimport torch.nn as nn\nfrom .layers import Self_Attn\nfrom .utils import  JDimPool\nfrom .resblocks import make_resblocks\nimport torch.nn.functional as F\n\n\nclass DropBlock2D(nn.Module):\n    def __init__(self, block_size, drop_prob):\n        super(DropBlock2D, self).__init__()\n        self.block_size = block_size\n        self.drop_prob = drop_prob\n\n    def forward(self, x, training_mode):\n        if training_mode != \"train\" or self.drop_prob == 0.0:\n            return x\n\n        # The probability of becoming the center of DropBlock\n        # Divided the patch size in case of selecting the same pixel as the center of the DropBlock\n        gamma = self.drop_prob * x.numel() / (self.block_size ** 2) / x[0].numel()\n\n        # Select the center pixel (H * W)\n        mask = (torch.rand(x.shape[0], *x.shape[2:], device=x.device) < gamma).float()\n\n        # Produce the DropBlock mask by expand the center to block_size * block_size (H * W)\n        mask = F.max_pool2d(mask.unsqueeze(1), kernel_size=self.block_size, stride=1, padding=self.block_size // 2)\n        mask = 1 - mask.squeeze(1)\n        drop_block_result = x * mask.unsqueeze(1) * (mask.numel() / mask.sum())\n        return drop_block_result\n\n\n\nclass CFOCNet(nn.Module):\n\n    def __init__(self, config):\n        super(CFOCNet, self).__init__()\n\n        self.config = config\n\n        self.res_q0, self.res_q1,self.res_q2,self.res_q3 = make_resblocks(config.data.deform_feature, data_pipe = \"query\")\n        self.res_r0, self.res_r1,self.res_r2,self.res_r3 = make_resblocks(config.data.deform_feature)\n\n        self.sa_q1 = Self_Attn(256, nn.ReLU()) # channel attention\n        self.sa_q2 = Self_Attn(512, nn.ReLU()) # channel attention\n        self.sa_q3 = Self_Attn(1024, nn.ReLU()) # channel attention\n\n        self.j_maxpool = JDimPool(7,1) # max pooling k reference image\n        \n        \n        self.maxpool_r1 = nn.MaxPool2d(4, stride=4, padding=0)\n        self.maxpool_r2 = nn.MaxPool2d(2, stride=2, padding=0)\n\n        self.match_query_conv1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1)\n        self.match_query_conv2 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1)\n        self.match_query_conv3 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1)\n\n        self.match_reference_conv1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1)\n        self.match_reference_conv2 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1)\n        self.match_reference_conv3 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1)\n\n\n        self.transpose_convolution = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding = 1)\n\n        self.dropblock0 = DropBlock2D(block_size=7, drop_prob=self.config.train.dropblock_prop)\n\n        self.fusion = nn.Sequential(\n            nn.Conv2d(3, 8, kernel_size=1, padding=0),  \n            nn.ReLU(),\n            nn.Conv2d(8, 4, kernel_size=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(4, 1, kernel_size=1, padding=0)  \n        )\n        \n        \n    # transform the feature vector into unit vector\n    def l2_normalize(self, x, dim=1, eps=1e-6):\n        return x / (x.norm(p=2, dim=dim, keepdim=True) + eps)\n    \n\n    def forward(self, queury, references, training_mode=\"val\"):\n\n        # query pipeline\n\n        q0 = self.res_q0(queury)\n\n        if self.config.train.dropblock : \n            q0 = self.dropblock0(q0, training_mode)\n        q1 = self.res_q1(q0)\n        q2 = self.res_q2(q1)\n        q3 = self.res_q3(q2)\n\n        q1_mix, _ = self.sa_q1(q1)\n        q2_mix, _ = self.sa_q2(q2)\n        q3_mix, _ = self.sa_q3(q3)\n\n        q1_norm_c = self.match_query_conv1(q1_mix)\n        q2_norm_c = self.match_query_conv2(q2_mix)\n        q3_norm_c = self.match_query_conv3(q3_mix)\n\n\n        # reference pipeline\n\n        org_ref_size = references.size()\n        references = references.view(-1,org_ref_size[-3],org_ref_size[-2],org_ref_size[-1])\n\n        r0 = self.res_r0(references)\n        r1 = self.res_r1(r0)\n        r2 = self.res_r2(r1)\n        r3 = self.res_r3(r2)\n\n        r1_size = r1.size()\n        r2_size = r2.size()\n        r3_size = r3.size()\n\n        r1 = r1.view(org_ref_size[0],org_ref_size[1],r1_size[-3],r1_size[-2],r1_size[-1])\n        r2 = r2.view(org_ref_size[0],org_ref_size[1],r2_size[-3],r2_size[-2],r2_size[-1])\n        r3 = r3.view(org_ref_size[0],org_ref_size[1],r3_size[-3],r3_size[-2],r3_size[-1])\n\n        r1_mix = self.j_maxpool(r1)\n        r2_mix = self.j_maxpool(r2)\n        r3_mix = self.j_maxpool(r3)\n\n\n        kernel1_norm_size = self.maxpool_r1(r1_mix)\n        kernel2_norm_size = self.maxpool_r2(r2_mix)\n        kernel3_norm_size = r3_mix\n\n        kernel1_norm_c = self.match_reference_conv1(kernel1_norm_size)\n        kernel2_norm_c = self.match_reference_conv2(kernel2_norm_size)\n        kernel3_norm_c = self.match_reference_conv3(kernel3_norm_size)\n\n\n\n        # image matching\n        \n        M1 = []\n        M2 = []\n        M3 = []\n\n        for i in range(kernel1_norm_c.size(0)):\n\n            if self.config.train.feature_l2_norm:\n                q1_feature, q2_feature, q3_feature = self.l2_normalize(q1_norm_c[i:i+1]), self.l2_normalize(q2_norm_c[i:i+1]), self.l2_normalize(q3_norm_c[i:i+1])\n                k1, k2, k3 = self.l2_normalize(kernel1_norm_c[i:i+1]), self.l2_normalize(kernel2_norm_c[i:i+1]), self.l2_normalize(kernel3_norm_c[i:i+1])\n            else : \n                q1_feature, q2_feature, q3_feature = q1_norm_c[i:i+1], q2_norm_c[i:i+1], q3_norm_c[i:i+1]\n                k1, k2, k3 = kernel1_norm_c[i:i+1], kernel2_norm_c[i:i+1], kernel3_norm_c[i:i+1]\n\n            \n            sample_m1 = nn.functional.conv2d(q1_feature, k1, padding=1, stride=2)\n            sample_m2 = nn.functional.conv2d(q2_feature, k2, padding=1, stride=2)\n            sample_m3 = nn.functional.conv2d(q3_feature, k3, padding=1, stride=2)\n            \n            M1.append(sample_m1)\n            M2.append(sample_m2)\n            M3.append(sample_m3)\n\n        M1 = torch.cat(M1,0)\n        M2 = torch.cat(M2,0)\n        M3 = torch.cat(M3,0)\n\n        M2 = nn.functional.interpolate(M2,scale_factor=2)\n        M3 = nn.functional.interpolate(M3,scale_factor=4)\n\n\n        # fusion different granularity\n        FS_gran = torch.cat([M1, M2, M3], dim=1)\n        FS = self.fusion(FS_gran)\n\n        FS = self.transpose_convolution(FS)\n\n        FS = nn.functional.interpolate(FS, scale_factor=4, mode='bilinear', align_corners=True)\n\n        return FS\n\n","metadata":{"_kg_hide-output":true,"cellView":"form","id":"t9eFoLtm2bk5","outputId":"928bbd1a-bf24-41f3-b547-211334bacc57","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile ./configs/config.yaml\n\n\n\ntrain:\n  epochs: 200\n  batch_size: 8\n  num_workers: 2\n  result_path: /kaggle/input/pandora-clean-straight-dataset-correct/360 CAC\n\n  ssim_loss: 1.0e-3\n  focal_weight : 15\n\n  feature_l2_norm : True # True/ False\n  dropblock : True # True/ False\n  dropblock_prop : 0.15\n\ndata:\n  data_path: .\n  num_references : 7\n  augmentation_type : \"soft\" # \"soft\"/\"hard\"\n  deform_feature : True # True/ False\n\noptimizer:\n  lr: 5.0e-5\n\neval:\n  checkpoint: /home/Hacker_Davinci/Desktop/Open_Source/CFOCNet/ckpt/model.ckpt\n  sample: True\n  image_folder: /home/Hacker_Davinci/Desktop/Open_Source/CFOCNet/image_folder\n  inference_time_test : False # True/ False\n  show_predict_image_amount : 2\n\n","metadata":{"id":"YgS_A3OJh5ms","outputId":"c4cb95a1-c30f-4239-ae46-dc3bf6c93600","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss.py\n\nimport torch.nn as nn\nfrom pytorch_msssim import ssim\nimport torch\nimport torch.nn.functional as F\n\nclass ObjectCountLoss : \n\n    def __init__(self, config):\n        self.config = config\n    \n\n    def focal_huber_loss(self, pred, target, delta = 1):\n    \n        focal_weight = torch.where(target > 0.1, self.config.train.focal_weight, 1.0)\n            \n        error = pred - target\n        abs_error = error.abs()\n    \n        large_error_num = (abs_error > delta).sum()\n        small_error_num = (abs_error <= delta).sum()\n    \n    \n        small_mask = (abs_error <= delta)\n        large_mask = (abs_error > delta)\n    \n        small_focal_weight = focal_weight[small_mask]\n        large_focal_weight = focal_weight[large_mask]\n        \n        small_loss = (error[small_mask] ** 2 + delta**2) / (2 * delta) * small_focal_weight\n        large_loss = abs_error[large_mask] * large_focal_weight\n    \n        \n        if large_error_num > 0:\n            large_loss = large_loss.sum() \n        else:\n            large_loss = torch.tensor(0.0, device=pred.device)\n    \n        if small_error_num > 0:\n            small_loss = small_loss.sum()\n        else:\n            small_loss = torch.tensor(0.0, device=pred.device)\n    \n    \n        loss = large_loss + small_loss\n    \n        \n        return loss\n\n    \n    \n    def compute_loss(self, predicted_density_map, ground_truth_density_map):\n    \n        Standard_L2_loss = self.focal_huber_loss(predicted_density_map, ground_truth_density_map)\n        \n        predicted_density_map = predicted_density_map.double()\n        ground_truth_density_map = ground_truth_density_map.double()\n        \n        SSIM_loss = 1 * self.config.train.batch_size - ssim(predicted_density_map, ground_truth_density_map, data_range=1.0, size_average=True)\n    \n        \n        Final_loss = Standard_L2_loss + self.config.train.ssim_loss * SSIM_loss\n    \n    \n        return Final_loss, SSIM_loss\n\n\n\n\n","metadata":{"cellView":"form","id":"a2AmIFdLOFjI","jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport numpy as np\nimport glob\nimport os\nimport cv2\nimport scipy.ndimage\nimport json\nimport random\nfrom tqdm import tqdm\nimport pickle\n\n\n\ncv2.setNumThreads(0) # disable multithread to avoid deadlocks\n\ndef add_latitude_channel(img_tensor):\n    \"\"\"\n    img_tensor: shape (B, 3, H, W) and H=256, W=256\n    return: (B, 4, H, W)\n    \"\"\"\n    B, C, H, W = img_tensor.shape\n    lat_values = torch.linspace(1, -1, steps=H).unsqueeze(1).repeat(1, W)  # (H, W)\n    \n    # broadcast to batch\n    lat_channel = lat_values.unsqueeze(0).repeat(B, 1, 1, 1)  # (B, H, W) → (B, 1, H, W)\n\n    # concat as 4th channel\n    img_with_lat = torch.cat([img_tensor, lat_channel], dim=1)\n    return img_with_lat\n\n\n\nclass CountingkDataset(Dataset):\n\n    def __init__(self, config, dataset_type, cat2imgkey):\n        super(CountingkDataset, self).__init__()\n        self.config = config\n        self.dataset_type = dataset_type\n        self.cat2imgkey = cat2imgkey\n        \n        self.query_transforms = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.reference_transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n\n        \n        if dataset_type == \"train\":\n          self.select_catIds = ['computer', 'car', 'chair', 'window', 'book', 'cabinet'] #354\n\n        elif dataset_type == \"val\":\n          self.select_catIds = ['door', 'penguin', 'bottle'] #53\n\n        elif dataset_type == \"test\":\n          self.select_catIds = ['dog', 'hyaenidae', 'boat', 'person']#103\n\n\n        \"\"\"\n            Choose the data by training mode\n        \"\"\"\n        \n        cache_dir = f\"/kaggle/working/cache_data/{dataset_type}\"\n        img2cat_path = os.path.join(cache_dir, \"img2cat.pkl\")\n        select_imgIds_path = os.path.join(cache_dir, \"select_imgIds.pkl\")\n        imgkey2reffnum_path = os.path.join(cache_dir, \"imgkey2reffnum.pkl\")\n        truenum_path = os.path.join(cache_dir, \"imgkey_imgnum2truenum.pkl\")\n\n        self.img2cat = {} \n        self.select_imgIds = [] \n        self.imgkey2reffnum = {}\n        self.imgkey_imgnum2truenum = {}\n\n        k = self.config.data.num_references\n\n\n        if all(os.path.exists(p) for p in [img2cat_path, select_imgIds_path, imgkey2reffnum_path, truenum_path]):\n\n            with open(img2cat_path, \"rb\") as f:\n                self.img2cat = pickle.load(f)\n            with open(select_imgIds_path, \"rb\") as f:\n                self.select_imgIds = pickle.load(f)\n            with open(imgkey2reffnum_path, \"rb\") as f:\n                self.imgkey2reffnum = pickle.load(f)\n            with open(truenum_path, \"rb\") as f:\n                self.imgkey_imgnum2truenum = pickle.load(f)\n\n            print(\"Dataset metadata loaded from cache.\")\n\n\n        else :\n\n\n          for c in self.select_catIds:\n            \n              imgs = self.cat2imgkey[c]\n              image_paths = glob.glob(f\"/kaggle/input/pandora-clean-straight-dataset-correct/360 CAC/refference_images/{c}/*.png\")\n            \n              refference_imgkey = []\n            \n              for path in image_paths:\n                  filename = os.path.basename(path)\n                  name_only = os.path.splitext(filename)[0]  \n                  refference_imgkey.append(name_only)\n              \n              \n              for i in imgs:\n                  self.img2cat[i] = c\n                  self.select_imgIds.append(i)\n\n                  select_reference_imgIds = np.random.choice(refference_imgkey,k,True)\n                  self.imgkey2reffnum[i] = select_reference_imgIds\n\n\n\n          for idx, imgkey in tqdm(enumerate(self.select_imgIds)):\n              selected_cat = self.img2cat[imgkey]\n              dm_np = np.loadtxt(f\"/kaggle/input/pandora-clean-straight-dataset-correct/360 CAC/density_maps/{selected_cat}/{imgkey}/0.txt\")\n              count = float(np.sum(dm_np))\n              self.imgkey_imgnum2truenum[(imgkey, \"0\")] = count\n\n          os.makedirs(cache_dir, exist_ok=True)\n          \n          with open(img2cat_path, \"wb\") as f:\n              pickle.dump(self.img2cat, f)\n          with open(select_imgIds_path, \"wb\") as f:\n              pickle.dump(self.select_imgIds, f)\n          with open(imgkey2reffnum_path, \"wb\") as f:\n              pickle.dump(self.imgkey2reffnum, f)\n          with open(truenum_path, \"wb\") as f:\n              pickle.dump(self.imgkey_imgnum2truenum, f)\n\n\n        \n        self.length = len(self.select_imgIds)\n\n\n\n    def __len__(self):\n        return self.length\n\n\n\n    def __getitem__(self, idx):\n\n        imgkey = self.select_imgIds[idx]\n        selected_cat = self.img2cat[imgkey]\n        \n        query_images_path = f\"/kaggle/input/pandora-clean-straight-dataset-correct/360 CAC/query_images/{selected_cat}/{imgkey}/0.png\"\n        img = cv2.imread(query_images_path, cv2.IMREAD_COLOR)\n        query_tensor = self.generate_query_tensor(img)\n\n        references_tensor = self.generate_references_tensor(selected_cat, self.imgkey2reffnum[imgkey])\n\n        density_maps_tensor = np.loadtxt(f\"/kaggle/input/pandora-clean-straight-dataset-correct/360 CAC/density_maps/{selected_cat}/{imgkey}/0.txt\")\n        density_maps_tensor = self.query_transforms(density_maps_tensor)\n\n        true_count = self.imgkey_imgnum2truenum.get((imgkey, \"0\"), 0.0)\n\n\n        \n        return query_tensor, references_tensor, density_maps_tensor, true_count, imgkey, selected_cat\n\n\n\n\n    def generate_query_tensor(self, img):\n        img_size = img.shape\n        max_img_size = max(img_size[0],img_size[1])\n        ph = int((max_img_size - img_size[0])/2)\n        pw = int((max_img_size - img_size[1])/2)\n\n        pad_img = np.pad(img,((ph,ph),(pw,pw),(0,0)))\n        pad_img = cv2.cvtColor(pad_img,cv2.COLOR_BGR2RGB)\n        query_tensor = self.query_transforms(pad_img)\n\n        return query_tensor\n\n\n\n    def generate_references_tensor(self,select_catId, select_reference_imgIds,k=5):\n\n        references_tensor = []\n\n        refference_path = f\"/kaggle/input/pandora-clean-straight-dataset-correct/360 CAC/refference_images/{select_catId}\"\n\n        for ref_id in select_reference_imgIds:\n            \n            crop_img = cv2.imread(os.path.join(refference_path,f'{ref_id}.png'),cv2.IMREAD_COLOR)\n            crop_img = cv2.cvtColor(crop_img,cv2.COLOR_BGR2RGB)\n            crop_tensor = self.reference_transform(crop_img)\n\n            references_tensor.append(crop_tensor)\n            \n        reference_concat = torch.stack(references_tensor, 0) # k reference image\n        \n        return reference_concat\n\n\n\n\n\n","metadata":{"cellView":"form","id":"6SE5gYszldDB","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Augmentation\n\nclass DataAugmentor:\n    def __init__(self, config):\n        self.augmentation_type = config.data.augmentation_type\n        self.deform_feature = config.data.deform_feature\n        \n        print(f\"apply {self.augmentation_type} data augmentation\")\n\n    def apply_jitter(self, img, brightness_factor, contrast_factor, saturation_factor, hue_factor):\n        img = T.functional.adjust_brightness(img, brightness_factor)\n        img = T.functional.adjust_contrast(img, contrast_factor)\n        img = T.functional.adjust_saturation(img, saturation_factor)\n        img = T.functional.adjust_hue(img, hue_factor)\n        return img\n\n    # horizontal flip\n    def flip(self, q_sample, r_sample, t_sample, direction = \"horizontal\"):\n        if direction == \"vertical\" : \n            q_aug = torch.flip(q_sample, dims=[1])  # flip height\n            r_aug = torch.flip(r_sample, dims=[2])\n            t_aug = torch.flip(t_sample, dims=[1])\n        else : \n            q_aug = torch.flip(q_sample, dims=[2])  # flip width\n            r_aug = torch.flip(r_sample, dims=[3])\n            t_aug = torch.flip(t_sample, dims=[2])\n        return q_aug, r_aug, t_aug\n\n    def rotation(self, q_sample, r_sample, t_sample, rotation_time=1):\n        q_aug = q_sample.rot90(rotation_time, [1,2])\n        r_aug = r_sample.rot90(rotation_time, [2,3])\n        t_aug = t_sample.rot90(rotation_time, [1,2])\n        return q_aug, r_aug, t_aug\n\n    def color_jitter(self, q_sample, r_sample, t_sample):\n        to_pil = T.ToPILImage()\n        to_tensor = T.ToTensor()\n\n        color_jitter = T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2)\n        \n        jitter_transform, brightness_factor, contrast_factor, saturation_factor, hue_factor = T.ColorJitter.get_params(\n            brightness=color_jitter.brightness,\n            contrast=color_jitter.contrast,\n            saturation=color_jitter.saturation,\n            hue=color_jitter.hue\n        )\n\n        \n        # do the same color jitter for every sample \n        r_aug_list = [to_tensor(self.apply_jitter(to_pil(r), brightness_factor, contrast_factor, saturation_factor, hue_factor)) for r in r_sample]\n        r_aug = torch.stack(r_aug_list, dim=0)\n\n        if self.deform_feature : \n            deform = q_sample[3:, :, :]                   \n            q_sample = q_sample[:3, :, :]\n        q_aug = to_tensor(self.apply_jitter(to_pil(q_sample), brightness_factor, contrast_factor, saturation_factor, hue_factor))\n        \n        if self.deform_feature : \n            q_aug = torch.cat([q_aug, deform], dim=0)\n        \n        t_aug = t_sample  # target don't need color jitter\n\n        return q_aug, r_aug, t_aug\n\n    def gaussian_blur(self, q_sample, r_sample, t_sample):\n        blur = T.GaussianBlur(kernel_size=(9,9), sigma=(2, 5))\n\n        if self.deform_feature : \n            deform = q_sample[3:, :, :]    # B, 1, H, W\n            q_sample = q_sample[:3, :, :]\n        q_aug = blur(q_sample)\n\n        if self.deform_feature : \n            q_aug = torch.cat([q_aug, deform], dim=0)\n        \n        r_aug = r_sample\n        t_aug = t_sample\n\n        return q_aug, r_aug, t_aug\n\n    def motion_blur(self, q_sample, r_sample, t_sample):\n        kernel_size = 9\n        kernel = torch.ones(1, 1, 1, kernel_size) / kernel_size  \n        \n        Cq = q_sample.shape[2]\n\n        if self.deform_feature : \n            deform = q_sample[3:, :, :]   \n            q_sample = q_sample[:3, :, :]\n        q = q_sample.permute(2, 0, 1).unsqueeze(0)  \n        q_aug = torch.nn.functional.conv2d(q, kernel.expand(Cq, 1, 1, kernel_size), padding=(0, kernel_size//2), groups=Cq)\n        q_aug = q_aug.squeeze(0).permute(1, 2, 0)\n        if self.deform_feature : \n            q_aug = torch.cat([q_aug, deform], dim=0)\n        \n        \n        t_aug = t_sample\n        r_aug = r_sample\n        \n        return q_aug, r_aug, t_aug\n\n    def apply_data_augmentation(self, queries, references, target):\n        if self.augmentation_type == \"soft\":\n            return self.apply_soft_data_augmentation(queries, references, target)\n        else:\n            return self.apply_hard_data_augmentation(queries, references, target)\n    \n\n    def apply_soft_data_augmentation(self, queries, references, target):\n\n        B, N, H, W, C = references.shape\n        augmented_queries = []\n        augmented_references = []\n        augmented_target = []\n\n        aug_type_index = -1\n        aug_type_title_list = [\"original\", \"horizontal flip\", \"vertical flip\", \"rotation 90\", \"rotation 180\", \"rotation 270\", \"color jitter\", \"gaussian blur\", \"motion blur \"]\n\n        for i in range(B):\n            q_sample = queries[i]  \n            r_sample = references[i]  \n            t_sample = target[i]  \n\n            aug_type = random.randint(0, 8)\n                \n\n            # keep original\n            if aug_type == 0:\n                q_aug, r_aug, t_aug = q_sample, r_sample, t_sample\n\n            # horizontal flip\n            elif aug_type == 1:\n                q_aug, r_aug, t_aug = self.flip(q_sample, r_sample, t_sample, direction = \"horizontal\")\n\n            # vertical flip\n            elif aug_type == 2:\n                q_aug, r_aug, t_aug = self.flip(q_sample, r_sample, t_sample, direction = \"vertical\")\n\n            # rotation 90\n            elif aug_type == 3:\n                q_aug, r_aug, t_aug = self.rotation(q_sample, r_sample, t_sample, rotation_time=1)\n\n            # rotation 180\n            elif aug_type == 4:\n                q_aug, r_aug, t_aug = self.rotation(q_sample, r_sample, t_sample, rotation_time=2)\n\n            # rotation 270\n            elif aug_type == 5:\n                q_aug, r_aug, t_aug = self.rotation(q_sample, r_sample, t_sample, rotation_time=3)\n\n            # color jitter\n            elif aug_type == 6:\n                q_aug, r_aug, t_aug = self.color_jitter(q_sample, r_sample, t_sample)\n\n            \n            # gaussian blur\n            elif aug_type == 7:\n                q_aug, r_aug, t_aug = self.gaussian_blur(q_sample, r_sample, t_sample)\n            \n            # motion blur \n            elif aug_type == 8:\n               q_aug, r_aug, t_aug = self.motion_blur(q_sample, r_sample, t_sample)\n                \n\n            augmented_queries.append(q_aug)\n            augmented_references.append(r_aug)\n            augmented_target.append(t_aug)\n\n        queries_aug = torch.stack(augmented_queries, dim=0)\n        references_aug = torch.stack(augmented_references, dim=0)\n        target_aug = torch.stack(augmented_target, dim=0)\n\n\n        return queries_aug, references_aug, target_aug\n\n\n    \n    def apply_hard_data_augmentation(self, queries, references, target, debug=False):\n        \n        B, N, H, W, C = references.shape\n        augmented_queries = []\n        augmented_references = []\n        augmented_target = []\n\n        i0_whether_h_flip = -1\n        i0_whether_v_flip = -1\n        i0_whether_rotation = -1\n        i0_whether_color_jitter = -1\n        i0_whether_blur = -1\n\n        \n        aug_type_title_list = [[\"original\", \"horizontal flip\"], [\"original\", \"vertical flip\"], [\"original\", \"rotation 90\", \"rotation 180\", \"rotation 270\"], [\"original\", \"color jitter\"], [\"original\",\"gaussian blur\", \"motion blur\"]]\n\n        \n        for i in range(B):\n            q_sample = queries[i]  \n            r_sample = references[i] \n            t_sample = target[i]  \n\n            whether_h_flip = random.randint(0, 1)\n            whether_v_flip = random.randint(0, 1)\n            whether_rotation = random.randint(0, 4)\n            whether_color_jitter = random.randint(0, 1)\n            whether_blur = random.randint(0, 3)\n\n\n            if i == 0 :\n                i0_whether_h_flip = whether_h_flip\n                i0_whether_v_flip = whether_v_flip\n                i0_whether_rotation = whether_rotation\n                i0_whether_color_jitter = whether_color_jitter\n                i0_whether_blur = whether_blur\n\n            # horizontal flip\n            if whether_h_flip == 1:\n                q_sample, r_sample, t_sample = self.flip(q_sample, r_sample, t_sample, direction = \"horizontal\")\n\n            # vertical flip\n            if whether_v_flip == 1:\n                q_sample, r_sample, t_sample = self.flip(q_sample, r_sample, t_sample, direction = \"vertical\")\n\n\n            # rotation 90\n            if whether_rotation == 1:\n                q_sample, r_sample, t_sample = self.rotation(q_sample, r_sample, t_sample, rotation_time=1)\n\n            # rotation 180\n            elif whether_rotation == 2:\n                q_sample, r_sample, t_sample = self.rotation(q_sample, r_sample, t_sample, rotation_time=2)\n\n            # rotation 270\n            elif whether_rotation == 3:\n                q_sample, r_sample, t_sample = self.rotation(q_sample, r_sample, t_sample, rotation_time=3)\n\n            # color jitter\n            if whether_color_jitter == 1:\n                q_sample, r_sample, t_sample = self.color_jitter(q_sample, r_sample, t_sample)\n                \n\n            # gaussian blur\n            if whether_blur == 1:\n                q_sample, r_sample, t_sample = self.gaussian_blur(q_sample, r_sample, t_sample)\n\n            # motion blur \n            elif whether_blur == 2:\n                q_sample, r_sample, t_sample = self.motion_blur(q_sample, r_sample, t_sample)\n            \n\n            augmented_queries.append(q_sample)\n            augmented_references.append(r_sample)\n            augmented_target.append(t_sample)\n\n        queries_aug = torch.stack(augmented_queries, dim=0)\n        references_aug = torch.stack(augmented_references, dim=0)\n        target_aug = torch.stack(augmented_target, dim=0)\n        \n\n        return queries_aug, references_aug, target_aug","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Traininer\n\nimport importlib\nimport os\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom torch import optim\nimport torch\nimport logging\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import save_image, make_grid\nimport model.CFOCNet\nimportlib.reload(model.CFOCNet)\nfrom model.CFOCNet import CFOCNet\nimport model.resblocks\nimportlib.reload(model.resblocks)\nfrom tqdm import tqdm\nfrom PIL import Image\nimport csv\nfrom datetime import datetime\nfrom torchvision.transforms import transforms\nfrom collections import deque\nimport copy\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nimport time\n\n\n\n\nclass Runner:\n    def __init__(self,args,config,logger):\n        self.args= args\n        self.config= config\n        self.logger = logger\n\n    def make_model_filename(self, global_step):\n        now = datetime.now().strftime(\"%m_%d_%H_%M\")\n        return f\"{now}_{global_step}_cfocnet.pt\"\n\n    def save_models(self, model, optimizer, global_step, save_dir=\"/kaggle/working/models\"):\n      os.makedirs(save_dir, exist_ok=True)\n      fname = self.make_model_filename(global_step)\n      path = os.path.join(save_dir, fname)\n      torch.save(\n          {\n              \"model_state\" : model.state_dict(),\n              \"optim_state\" : optimizer.state_dict()\n          }, path\n      )\n\n      return fname\n\n    def load_models(self,  model, model_name, optimizer=\"\", save_dir=\"/kaggle/input/09_02_03_49_1400_cfocnet/pytorch/default/1\"):\n\n      checkpoint = torch.load(os.path.join(save_dir, f\"{model_name}_cfocnet.pt\"))\n      model.load_state_dict(checkpoint[\"model_state\"])\n      if optimizer != \"\" :\n          optimizer.load_state_dict(checkpoint[\"optim_state\"])\n\n\n    def show_predicted_output(self, queries, target, FS, evaluation_mode):\n      print(f\"{evaluation_mode} output : \")\n\n      if self.config.data.deform_feature : \n          img1 = queries[0][:3, :, :].detach().cpu().numpy().transpose(1,2,0)\n      else :\n          img1 = queries[0].detach().cpu().numpy().transpose(1,2,0)\n      img2 = target[0].detach().cpu().numpy().transpose(1,2,0)\n      img3 = F.relu(FS[0]).detach().cpu().numpy().transpose(1,2,0)\n      show_images  = [img1, img2, img3]\n      \n      fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n      for ax, img in zip(axes, show_images):\n          ax.imshow(img)\n          ax.axis('off')  \n      \n      plt.tight_layout()\n      plt.show()\n\n      \n\n    def evaluate_by_metrics(self, true_count, FS):\n\n        count_num_list = [round(this_true_count.item(), 2) for this_true_count in true_count]    \n        pred_object_num = [round(torch.sum(F.relu(FS_i)).item(), 2) for FS_i in FS]\n\n        count_num_list = np.array(count_num_list)\n        pred_object_num = np.array(pred_object_num)\n\n        this_mae = np.sum(np.abs(count_num_list - pred_object_num))\n        this_mse = np.sum((count_num_list - pred_object_num) **2)\n        this_nae = np.sum(np.abs(count_num_list - pred_object_num) / count_num_list)\n        this_sre = np.sum(((count_num_list - pred_object_num) **2) / count_num_list)\n\n        return this_mae, this_mse, this_nae, this_sre\n\n\n    \n    def train(self, cat2imgkey, selected_model_name, selected_step = 1 , val_step=500, store_step = 1000, valid_val_step=5000):\n        tqdm._instances.clear()\n      \n        # Import the dataset\n        dataset = CountingkDataset(self.config,'train', cat2imgkey) \n        data_loader = DataLoader(dataset,self.config.train.batch_size, shuffle=True, pin_memory=True)\n        print(\"dataset length \", len(dataset))\n        print(f\"1 epoch = {len(dataset)//self.config.train.batch_size + 1} batch\")\n        data_augmentor = DataAugmentor(self.config)\n        \n\n        net = CFOCNet(self.config)\n        net.to(self.config.device)\n        optimizer = optim.Adam(net.parameters(),lr=self.config.optimizer.lr)\n        \n        if selected_model_name != \"\":\n          self.load_models(net, selected_model_name, optimizer=optimizer, save_dir=f\"/kaggle/input/{selected_model_name}_cfocnet/pytorch/default/1\") \n        \n\n        global_step = 1\n\n\n        best_nae = 1000000000000000000\n        best_sre = 1000000000000000000\n\n        my_loss = ObjectCountLoss(self.config)\n\n        total_steps = self.config.train.epochs * len(data_loader)\n        pbar = tqdm(total=total_steps, desc=\"Training\", unit=\"step\")\n\n\n        for epoch in range(1, self.config.train.epochs + 1):\n            \n            \n            for sample in data_loader:\n\n                if global_step < selected_step :\n                    pbar.set_description(f\"Epoch {epoch}\")\n                    pbar.update(1)\n                    global_step += 1\n                    continue\n\n                optimizer.zero_grad()\n                net.train()\n                \n                queries = sample[0] # B * Idx * H * W * C\n                references = sample[1] # B * H * W * C\n                target = sample[2] # B * Idx * H * W * C\n                true_count = sample[3].to(self.config.device) # B * Idx * H * W * C\n                imgkey = sample[4]\n                selected_cat = sample[5]\n\n\n                B, N = queries.shape[0], queries.shape[1]\n\n                if self.config.data.deform_feature : \n                    queries = add_latitude_channel(queries)\n                \n                queries, references, target = data_augmentor.apply_data_augmentation(queries, references, target)\n                \n                queries = queries.to(self.config.device)\n                references = references.to(self.config.device)\n                target = target.to(self.config.device)\n                    \n                \n                FS = net(queries, references, \"train\")\n\n                \n                Final_loss, SSIM_loss = my_loss.compute_loss(FS, target)\n                \n\n\n                self.logger.info(f\"Step : {global_step} Loss: {Final_loss.item()}\")\n\n                \n                Final_loss.backward()\n                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=10.0)\n\n                optimizer.step()\n\n\n                \n                \n\n                if global_step >= valid_val_step  and  global_step % val_step == 0 :\n\n                  this_mae, this_mse, this_nae, this_sre = self.evaluate_by_metrics(true_count, FS)\n\n                  self.show_predicted_output(queries, target, FS, \"train\")\n                    \n                  b_size = len(FS)\n                  print(f\"imgkey : {imgkey[0]}\")\n                  print(f\"count category : {selected_cat[0]}\")\n                  print(f'nae = {this_nae/b_size}')\n                  print(f'sre = {this_sre/b_size}')\n                  print(\"\\n\")\n\n\n                  val_nae, val_sre = self.test(\"val\", cat2imgkey, net= net, whether_show = True)\n                  \n                  if (val_nae < best_nae) and (val_sre < best_sre):\n                      model_names = self.save_models(net, optimizer, global_step)\n                      best_nae = val_nae\n                      best_sre = val_sre\n                      print(f\"we have best nae : {val_nae} and sre: {val_sre}\")\n            \n\n                if global_step % store_step == 0 :\n                    model_names = self.save_models(net, optimizer, global_step)\n\n                pbar.set_description(f\"Epoch {epoch}\")\n                pbar.update(1)\n\n\n                global_step+=1\n\n\n\n    \n    def test(self, evaluation_mode, cat2imgkey, net= \"\", selected_model_name=\"\", whether_show=False, inference_time_test=False):\n\n        if selected_model_name != \"\":\n          net = CFOCNet(self.config)\n          self.load_models(net, selected_model_name, save_dir=f\"/kaggle/input/{selected_model_name}_cfocnet/pytorch/default/1\") \n\n        \n        net.to(self.config.device)\n        net.eval()\n\n        \n        group_dataset = CountingkDataset(self.config, evaluation_mode, cat2imgkey) \n        data_loader = DataLoader(group_dataset, self.config.train.batch_size)\n\n\n        mae_sum, mse_sum, nae_sum, sre_sum = 0, 0, 0, 0\n\n\n        count = len(group_dataset)\n        \n\n        plt_query_images = []\n        plt_target_images = []\n        plt_predict_images = []\n\n        start_time = time.time()\n\n        with torch.no_grad():\n            for i, sample in enumerate(data_loader):\n\n                queries = sample[0]\n                references = sample[1].to(self.config.device) # B * H * W * C\n                target = sample[2].to(self.config.device) # B * Idx * H * W * C\n                true_count = sample[3].to(self.config.device) # B * Idx * H * W * C\n                imgkey = sample[4]\n                selected_cat = sample[5]\n                \n\n\n                if self.config.data.deform_feature : \n                    queries = add_latitude_channel(queries)\n                \n                queries = queries.to(self.config.device)\n\n\n                \n                FS = net(queries, references)\n\n                this_mae, this_mse, this_nae, this_sre = self.evaluate_by_metrics(true_count, FS)\n\n\n                mae_sum += this_mae\n                mse_sum += this_mse\n                nae_sum += this_nae\n                sre_sum += this_sre\n\n                \n                if i < self.config.eval.show_predict_image_amount : # select some sample and show predict image\n                    self.show_predicted_output(queries, target, FS, evaluation_mode)\n                    \n                    print(f\"imgkey : {imgkey[0]}\")\n                    print(f\"count category : {selected_cat[0]}\")\n                    print(f'this nae = {this_nae}')\n                    print(f'this sre = {this_sre}')  \n\n                    print(\"\\n\")\n                  \n                  \n\n\n\n        end_time = time.time()\n\n        if self.config.eval.inference_time_test : \n            total_time = end_time - start_time\n            total_images = len(group_dataset)\n            avg_time_per_image = total_time / total_images\n            print(f\"Average time per image : {avg_time_per_image:.6f} sec ({1/avg_time_per_image:.2f} FPS)\")\n\n\n        # print(f'mae = {mae_sum/count}')\n        # print(f'mse = {mse_sum/count}')\n        print(f'nae = {nae_sum/count}')\n        print(f'sre = {sre_sum/count}')\n\n\n        return nae_sum/count, sre_sum/count\n        \n\n\n\n","metadata":{"cellView":"form","id":"qb1kaFJCBm_v","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Function\n\nimport torch\nimport logging\nimport os\nimport sys\nimport shutil\nimport numpy as np\nimport argparse\nimport yaml \nimport copy\nfrom datetime import datetime\nimport sys\n\n\n\ndef parse_args_and_config():\n\n    # args\n\n    parser = argparse.ArgumentParser(description=globals()['__doc__'])\n\n    parser.add_argument('--config', type=str, required=True,  help='Path to the config file')\n    parser.add_argument('--seed', type=int, default=0, help='Random seed')\n    parser.add_argument('--exp', type=str, default='exp', help='Path for saving running related data.')\n    parser.add_argument('--doc', type=str, required=True, help='A string for documentation purpose. '\n                                                               'Will be the name of the log folder.')\n    parser.add_argument('--comment', type=str, default='', help='A string for experiment comment')\n    parser.add_argument('--verbose', type=str, default='info', help='Verbose level: info | debug | warning | critical')\n    parser.add_argument('--train', action='store_true', help='Whether to train the model')\n    parser.add_argument('--val', action='store_true', help='Whether to val the model')\n    parser.add_argument('--test', action='store_true', help='Whether to test the model')\n    parser.add_argument('--sample', action='store_true', help='Whether to produce samples from the model')\n    parser.add_argument('--resume_training', action='store_true', help='Whether to resume training')\n    parser.add_argument('-i', '--image_folder', type=str, default='images', help=\"The folder name of samples\")\n\n    args = parser.parse_args()\n\n    args.log_path = os.path.join(args.exp, 'logs', args.doc)\n\n    # set random seed\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(args.seed)\n    \n\n    # config\n    with open(os.path.join('configs', args.config), 'r') as f:\n        config = yaml.load(f,yaml.CLoader)\n    new_config = dict2namespace(config)\n\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    new_config.device = device\n    \n    \n    os.makedirs(\"/kaggle/working/logging_files\", exist_ok=True)\n\n\n    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = f\"/kaggle/working/logging_files/log_file_{now}.txt\"\n    logger = logging.getLogger(\"train_logger\")\n    logger.setLevel(logging.INFO)\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setFormatter(logging.Formatter('%(message)s'))\n    logger.addHandler(file_handler)\n  \n\n    # add device\n    logging.info(\"Using device: {}\".format(device))\n\n\n    return args, new_config, logger\n\n\n\"\"\"\n    we can use config.class.subclass to get config\n\"\"\"\ndef dict2namespace(config):\n    namespace = argparse.Namespace()\n    for key, value in config.items():\n        if isinstance(value, dict):\n            new_value = dict2namespace(value)\n        else:\n            new_value = value\n        setattr(namespace, key, new_value)\n    return namespace\n\n\n\n\ndef main(cat2imgkey, selected_model_name, selected_step):\n\n    sys.argv = [\n      '',  \n      '--config', 'config.yaml',\n      '--doc', 'traininglog',\n      '--train',\n      # '--val',\n      # '--test',\n    ]\n\n\n    args, config, logger = parse_args_and_config()\n    \n    config_dict = copy.copy(vars(config)) # vars\n    \n    print(config)\n\n    runner = Runner(args,config,logger)\n    if args.test:\n        runner.test(\"test\", cat2imgkey, selected_model_name = selected_model_name)\n    elif args.val:\n        runner.test(\"val\", cat2imgkey, selected_model_name = selected_model_name)\n    elif args.train:\n        runner.train(cat2imgkey, selected_model_name, selected_step=selected_step, val_step=500, store_step = 1000, valid_val_step=500)\n    else:\n        print(\"Add --test or --train in your command line or run.sh !\")\n\n\n\n\nif __name__ == '__main__':\n    try:\n        selected_model_name = \"\" # model name like : 10_17_06_57_10 #if you want to train the model from scratch, set \"\"\n        selected_step = 1 # if you want to train the model from scratch, set 1\n        main(cat2imgkey, selected_model_name, selected_step)\n    except Exception as e:\n        print(f\"Error : {e}\")\n","metadata":{"cellView":"form","id":"vsySc0GjBzYY","outputId":"d144208f-cd87-4b74-9436-ab957756d5db","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}